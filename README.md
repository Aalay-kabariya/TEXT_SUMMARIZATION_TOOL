# TEXT_SUMMARIZATION_TOOL
# Abstractive Text Summarization Using BART and Streamlit  
This project focuses on **abstractive text summarization** using **BART (Bidirectional and Auto-Regressive Transformers)** from **Hugging Faceâ€™s Transformers** library. The goal is to train and deploy a model that reads long documents and generates **concise, human-like summaries**. To make it accessible, a **Streamlit web application** has been developed, enabling seamless user interaction.

## Project Details  
- **Company:** CODETECH IT SOLUTIONS  
- **Intern Name:** Aalay Kabariya 
- **Intern ID:** C0DF49  
- **Domain:** Artificial Intelligence  
- **Duration:** 4 Weeks  
- **Mentor:** Neela Santosh  

## What Was Done  
âœ” **Dataset Preparation** â€“ Loaded and split the **XSum** dataset using **Hugging Face's datasets** library.  
âœ” **Model Initialization** â€“ Used the **facebook/bart-base** pre-trained model and tokenizer.  
âœ” **Data Preprocessing** â€“ Tokenized documents and summaries with padding and truncation.  
âœ” **Training Configuration** â€“ Fine-tuned using **Hugging Faceâ€™s Trainer API** over 3 epochs.  
âœ” **Model Evaluation** â€“ Assessed performance using **ROUGE metrics**.  
âœ” **Streamlit App Integration** â€“ Developed an **interactive UI** for real-time summarization.  

## Tools and Technologies Used  
- **Python** â€“ Core programming language  
- **BART Model (facebook/bart-base)** â€“ Transformer-based summarization model  
- **Hugging Face datasets** â€“ Dataset management  
- **Hugging Face transformers** â€“ Pre-trained models & tokenizers  
- **Trainer API & TrainingArguments** â€“ Model fine-tuning utilities  
- **ROUGE Metrics** â€“ Summary evaluation  
- **Streamlit** â€“ Web app framework  
- **PyTorch & CUDA** â€“ Hardware acceleration  

## Process and Workflow  
### **Dataset Selection**  
- Used **XSum dataset**, ideal for abstractive summarization tasks.  

### **Tokenization & Preprocessing**  
- Applied **BartTokenizer** with max-length constraints:  
  - **Documents:** 256 tokens  
  - **Summaries:** 64 tokens  

### **Model Training**  
- Utilized **Trainer class** for streamlined training with:  
  - **Batch size:** 2  
  - **Epochs:** 3  
  - **Logging & evaluation strategies**  
  - **DataCollatorForSeq2Seq** for dynamic padding  

### **Model Deployment**  
- Saved fine-tuned **BART model** and deployed it via **Streamlit**.  
- Users input custom text and receive **concise AI-generated summaries**.  

### **Evaluation**  
- Computed **ROUGE scores** on test set to validate model accuracy.  

## Output  
### Sample Input  
ðŸ–¼ _Original Document_  

### Generated Summary  
ðŸ–¼ _Summarized Text_  

## Installation  
```bash
git clone https://github.com/yourusername/text-summarization.git
cd text-summarization
pip install torch transformers datasets streamlit evaluate
streamlit run summarization-app.py  # Launch the app
